# -*- coding: utf-8 -*-
"""Phase 3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JdkCecLL8WtHINK2LdrFeITCS_gS6QhH
"""

#mapping anotations
import numpy as np
import pandas as pd
import pywt
from sklearn.preprocessing import MinMaxScaler
import os
import matplotlib.pyplot as plt


# Configure plots (if needed later)
plt.rcParams["figure.figsize"] = (12, 6)
plt.rcParams['lines.linewidth'] = 1
plt.rcParams['axes.grid'] = True

# Beat classification function based on AAMI classes and MIT-BIH labels
def get_beat_class(label):
    if label in ["N", "L", "R", "e", "j"]:
        return 0  # "N" class
    elif label in ["S", "A", "J", "a"]:
        return 1  # "S" class
    elif label in ["V", "E"]:
        return 2  # "V" class
    elif label == "F":
        return 3  # "F" class
    elif label in ["/", "f", "Q"]:
        return 4  # "Q" class
    return -1  # "None" class for unrecognized labels

def process_annotations(annotation_file):
    df = pd.read_csv(annotation_file, sep='\s+', skipinitialspace=True, engine='python',
                     names=['Time', 'Sample #', 'Type', 'Sub', 'Chan', 'Num', 'Aux'], skiprows=1, on_bad_lines='skip')
    df['labeled_type'] = df['Type'].apply(get_beat_class)
    col_del = ['Time', 'Sub', 'Chan', 'Num', 'Aux']
    df.drop(col_del, axis=1, inplace=True)
    return df


# Path to the segmented, denoised, and normalized data
input_dir = '/content/drive/My Drive/ecg_classification_master/denoised'
output_dir = '/content/drive/My Drive/ecg_classification_master/denoised_labeled'
annotation_path = '/content/drive/My Drive/ecg_classification_master/dataset'

os.makedirs(output_dir, exist_ok=True)

# List of dataset IDs
training_set_ids = [100,104,114,118,124,203,208,213,223,230]
try:
    filenames = next(os.walk(input_dir))[2]
except StopIteration:
    print("Directory is empty or path is incorrect.")
    filenames = []

if not filenames:
    print("No segmented denoised CSV files found.")
else:
    for filename in filenames:
        file_path = os.path.join(input_dir, filename)
        dataset_id = filename.split('_')[0]

        if int(dataset_id) not in training_set_ids:
            continue

        print(f"Processing file {filename}")

        # Load the segmented denoised data
        segments_df = pd.read_csv(file_path, header=None)

        # Process the annotations
        annotation_file = os.path.join(annotation_path, f'{dataset_id}annotations.txt')
        annotations = process_annotations(annotation_file)

        # Segment the denoised and normalized data into 1000-sample segments
        segment_size = 1000
        num_segments = len(segments_df) // segment_size

        # Collect segments and their corresponding labels
        segments = segments_df.values
        labels = []

        for i in range(len(segments)):
            start_idx = i * segment_size
            end_idx = start_idx + segment_size

            # Identify the label for the segment
            segment_labels = annotations[(annotations['Sample #'] >= start_idx) & (annotations['Sample #'] < end_idx)]
            if not segment_labels.empty:
                label = segment_labels['labeled_type'].mode()[0]  # Use the most frequent label in the segment
            else:
                label = -1  # No label
            labels.append(label)

        # Convert the list of segments into a DataFrame
        labels_df = pd.DataFrame(labels, columns=['Label'])

        # Concatenate segments and labels into one DataFrame
        result_df = pd.concat([segments_df, labels_df], axis=1)

        # Save the result to a CSV file
        result_file_path = os.path.join(output_dir, f'{dataset_id}_denoised_labeled.csv')
        result_df.to_csv(result_file_path, header=False, index=False)

        # Print summary table
        label_summary = labels_df['Label'].value_counts().sort_index()
        summary_table = pd.DataFrame({'Class': label_summary.index, 'Count': label_summary.values})
        print(summary_table)

        print(f"Normalized, denoised, and labeled segmented data has been saved to {result_file_path}")  # Collect segments and their corresponding labels
        segments = segments_df.values
        labels = []

        for i in range(len(segments)):
            start_idx = i * segment_size
            end_idx = start_idx + segment_size

            # Identify the label for the segment
            segment_labels = annotations[(annotations['Sample #'] >= start_idx) & (annotations['Sample #'] < end_idx)]
            if not segment_labels.empty:
                label = segment_labels['labeled_type'].mode()[0]  # Use the most frequent label in the segment
            else:
                label = -1  # No label
            labels.append(label)

        # Convert the list of segments into a DataFrame
        labels_df = pd.DataFrame(labels, columns=['Label'])

        # Update the total class counts
        for class_label in total_class_counts:
            total_class_counts[class_label] += (labels_df['Label'] == class_label).sum()

        # Concatenate segments and labels into one DataFrame
        result_df = pd.concat([segments_df, labels_df], axis=1)

        # Save the result to a CSV file
        result_file_path = os.path.join(output_dir, f'{dataset_id}_denoised_labeled.csv')
        result_df.to_csv(result_file_path, header=False, index=False)

        # Print summary table for this file
        label_summary = labels_df['Label'].value_counts().sort_index()
        summary_table = pd.DataFrame({'Class': label_summary.index, 'Count': label_summary.values})
        print(summary_table)

        print(f"Normalized, denoised, and labeled segmented data has been saved to {result_file_path}")

# Print the total counts for each class across all files
print("\nTotal count for each class across all files:")
total_count_table = pd.DataFrame({'Class': total_class_counts.keys(), 'Total Count': total_class_counts.values()})
print(total_count_table)